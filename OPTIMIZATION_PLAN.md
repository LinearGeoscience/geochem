# üöÄ Drillhole Processing Performance Optimization Plan

## Executive Summary
Your 125MB dataset (48K drillholes, 298K assays, 225K surveys) is slow because of **inefficient algorithms**, not the math itself. The current implementation has **O(M√óN) complexity** where it should be **O(M+N)**. We can achieve **10-100x speedup** with targeted optimizations.

## üî¥ CRITICAL BOTTLENECKS (Causing 90% of Slowdown)

### 1. **Nested DataFrame Filtering in Loop**
**Location:** `backend/app/core/drillhole_manager.py:40-103`
```python
# CURRENT (SLOW) - O(M√óN) complexity
for hole_id in collar_df[hole_col].unique():  # 48,278 iterations
    assays = assay_df[assay_df[hole_col] == hole_id]  # Scans 298K rows each time!
    # This does 48,278 √ó 298,631 = 14.4 BILLION comparisons!
```

**SOLUTION:** Use groupby (10-100x faster)
```python
# OPTIMIZED - O(M+N) complexity
assay_groups = assay_df.groupby(hole_col)
survey_groups = survey_df.groupby(hole_col)
for hole_id in collar_df[hole_col].unique():
    assays = assay_groups.get_group(hole_id)  # Instant lookup!
```
**Impact:** Reduce from 14.4 billion to 298K operations

### 2. **Unnecessary Disk I/O**
**Location:** `backend/app/api/data.py:50-58`
```python
# CURRENT - Writes to disk, reads back, deletes (3 times!)
with open(path, "wb") as buffer:
    shutil.copyfileobj(file.file, buffer)  # Write 125MB to disk
df = pd.read_csv(path)  # Read 125MB from disk
os.remove(path)
```

**SOLUTION:** Read directly in memory
```python
# OPTIMIZED - No disk I/O
content = file.file.read()
df = pd.read_csv(io.BytesIO(content))
```
**Impact:** Eliminate 250MB of disk I/O

## üìä Performance Impact Analysis

| Optimization | Current Time | After Optimization | Speedup |
|-------------|--------------|-------------------|---------|
| DataFrame filtering | ~60 seconds | ~2 seconds | **30x** |
| File I/O | ~10 seconds | ~1 second | **10x** |
| Type detection | ~5 seconds | ~0.5 seconds | **10x** |
| **TOTAL** | **~75+ seconds** | **~3-5 seconds** | **15-25x** |

## üéØ Implementation Plan (3 Phases)

### **Phase 1: Quick Wins (30 minutes, 10x speedup)**
These require minimal code changes but massive impact:

1. **Replace loop filtering with groupby** (10 mins)
   - File: `drillhole_manager.py`
   - Lines: 40-103
   - Expected speedup: 10-30x

2. **Eliminate disk I/O** (5 mins)
   - File: `data.py`
   - Lines: 50-58
   - Expected speedup: 2-3x

3. **Consolidate type detection** (5 mins)
   - File: `data.py`
   - Lines: 71-75
   - Expected speedup: 3x

4. **Add progress reporting** (10 mins)
   - Show real progress in terminal
   - Add timing logs

### **Phase 2: Algorithm Optimizations (1 hour, additional 2-3x)**

1. **Vectorize desurvey calculations**
   - Batch process all holes at once using NumPy
   - Pre-allocate result arrays

2. **Optimize column matching**
   - Pre-compile regex patterns
   - Use set operations instead of loops

3. **Memory optimization**
   - Use categorical dtypes for hole IDs
   - Process in chunks if needed

### **Phase 3: Architecture Changes (Optional, for production)**

1. **Parallel Processing**
   ```python
   from concurrent.futures import ProcessPoolExecutor
   with ProcessPoolExecutor() as executor:
       results = executor.map(process_hole, hole_groups)
   ```
   - Process multiple holes simultaneously
   - Could achieve 4-8x additional speedup on multi-core

2. **Streaming Upload**
   - Process files as they upload
   - Show real-time progress

3. **Caching Layer**
   - Cache desurveyed results
   - Only reprocess changed data

## üîß Specific Code Changes

### Change 1: Optimize Main Desurvey Loop
```python
# backend/app/core/drillhole_manager.py

def desurvey_drillholes_optimized(collar_df, survey_df, assay_df):
    # Pre-group dataframes (single pass through data)
    assay_groups = assay_df.groupby(hole_col)
    survey_groups = survey_df.groupby(hole_col)
    collar_indexed = collar_df.set_index(hole_col)

    # Pre-allocate result list
    results = []
    total_holes = len(collar_df)

    # Process each hole (no repeated filtering!)
    for i, hole_id in enumerate(collar_indexed.index):
        if i % 100 == 0:  # Progress reporting
            print(f"Processing hole {i}/{total_holes} ({i*100/total_holes:.1f}%)")

        collar = collar_indexed.loc[hole_id]
        surveys = survey_groups.get_group(hole_id).sort_values(depth_col)
        assays = assay_groups.get_group(hole_id).sort_values([from_col, to_col])

        # ... rest of desurvey logic (unchanged)
```

### Change 2: Optimize File Reading
```python
# backend/app/api/data.py

import io

async def read_file_optimized(file: UploadFile) -> pd.DataFrame:
    # Read entire file into memory once
    content = await file.read()

    # Parse based on file type
    if file.filename.endswith('.csv'):
        df = pd.read_csv(io.BytesIO(content),
                        low_memory=False,  # Faster for known types
                        dtype_backend='numpy_nullable')  # Better NaN handling
    else:
        df = pd.read_excel(io.BytesIO(content))

    # Optimize memory usage
    df = df.convert_dtypes()  # Use best dtypes
    return df
```

### Change 3: Batch Column Processing
```python
# backend/app/core/data_manager.py

def detect_column_types_optimized(self):
    # Single pass type detection
    numeric_cols = self.df.select_dtypes(include=[np.number]).columns
    text_cols = self.df.select_dtypes(exclude=[np.number]).columns

    self.column_types = {col: "numeric" for col in numeric_cols}
    self.column_types.update({col: "text" for col in text_cols})

    # Cache cleaned column names
    self._col_cache = {col: col.lower().strip() for col in self.df.columns}
```

## üìà Expected Results

### Before Optimization:
- Upload: 10-20 seconds
- Processing: 60+ seconds
- Total: **75-90 seconds** ‚ùå

### After Phase 1:
- Upload: 2-3 seconds
- Processing: 3-5 seconds
- Total: **5-8 seconds** ‚úÖ

### After Phase 2:
- Upload: 1-2 seconds
- Processing: 1-2 seconds
- Total: **2-4 seconds** üöÄ

## üîë Key Insights

1. **The math is fast** - NumPy trigonometry takes microseconds
2. **The data access is slow** - Repeated DataFrame filtering is the killer
3. **Memory is faster than disk** - Avoid file I/O when possible
4. **Groupby > Loop filtering** - Always use pandas groupby for grouped operations
5. **Vectorization wins** - Batch operations are 10-100x faster than loops

## üìù Testing Strategy

1. **Create test dataset:**
   - Small (100 holes): Verify correctness
   - Medium (5K holes): Benchmark improvements
   - Large (50K holes): Stress test

2. **Profile each change:**
   ```python
   import time
   start = time.time()
   result = desurvey_drillholes(...)
   print(f"Time: {time.time() - start:.2f}s")
   ```

3. **Monitor memory:**
   ```python
   import psutil
   process = psutil.Process()
   print(f"Memory: {process.memory_info().rss / 1024 / 1024:.1f} MB")
   ```

## ‚úÖ Quick Decision

**Should we proceed with Phase 1?**
- Time investment: 30 minutes
- Expected speedup: 10-15x
- Risk: Very low (simple refactoring)
- Result: 75 seconds ‚Üí 5 seconds

This will make your 125MB dataset process in **under 5 seconds** instead of timing out!